{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453ea87e",
   "metadata": {},
   "source": [
    "# Semi-supervised BERTopic Modeling with Probability Calculation\n",
    "https://maartengr.github.io/BERTopic/getting_started/semisupervised/semisupervised.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aae420",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befd5d2b-fd9b-41cd-a29c-5ace51820c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiolf1/anaconda3/envs/BERTopic/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=numba.NumbaDeprecationWarning)\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4bc4f",
   "metadata": {},
   "source": [
    "## Import and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2a363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_businesses.tsv', sep='\\t', dtype={'BusinessShortNumber': str})\n",
    "\n",
    "# Split text and TagNames\n",
    "docs = df[[\"BusinessShortNumber\", \"all\"]]\n",
    "categories = df[\"TagNames\"]\n",
    "\n",
    "# Convert categories to numbers\n",
    "categories = pd.factorize(categories)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7d52a5-0c0e-4375-bf49-5d325de7527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "docs['all'] = docs['all'].apply(lambda doc: re.sub(r'[^\\w\\s]', '', doc))\n",
    "\n",
    "\n",
    "# Lowercase\n",
    "docs['all'] = docs['all'].apply(lambda doc: doc.lower())\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Import custom stopwords file as list of strings\n",
    "with open('../../data/custom_stopwords.txt', 'r') as f:\n",
    "   custom_stopwords = f.readlines()\n",
    "\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "custom_stopwords = [x.strip() for x in custom_stopwords]\n",
    "\n",
    "# remove stopwords from docs\n",
    "docs['all'] = docs['all'].apply(lambda doc: ' '.join(word for word in doc.split() if word not in german_stop_words))\n",
    "docs['all'] = docs['all'].apply(lambda doc: ' '.join(word for word in doc.split() if word not in custom_stopwords))\n",
    "\n",
    "# remove \"na\" from docs\n",
    "docs = docs[docs['all'] != 'na']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514b5a8",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d83821-afbe-4c5c-a86e-0a19881291fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 21/21 [03:08<00:00,  8.99s/it]\n",
      "2023-08-27 16:55:02,198 - BERTopic - Transformed documents to Embeddings\n",
      "2023-08-27 16:55:10,961 - BERTopic - Reduced dimensionality\n",
      "2023-08-27 16:55:11,372 - BERTopic - Clustered reduced embeddings\n",
      "2023-08-27 16:55:14,564 - BERTopic - Reduced number of topics from 168 to 140\n",
      "Batches: 100%|██████████| 21/21 [03:14<00:00,  9.24s/it]\n",
      "2023-08-27 16:58:29,596 - BERTopic - Transformed documents to Embeddings\n",
      "2023-08-27 16:58:33,378 - BERTopic - Reduced dimensionality\n",
      "2023-08-27 16:58:33,910 - BERTopic - Clustered reduced embeddings\n",
      "2023-08-27 16:58:37,707 - BERTopic - Reduced number of topics from 179 to 140\n"
     ]
    }
   ],
   "source": [
    "# https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#hdbscan\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size = 2,\n",
    "    min_samples = 1,\n",
    "    metric = 'euclidean',\n",
    "    prediction_data = True)\n",
    "\n",
    "# BERTopic German model\n",
    "# Parameter tuning: https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#bertopic\n",
    "topic_model = BERTopic(\n",
    "    language = \"multilingual\",\n",
    "    min_topic_size = 2, # minimum number of documents in a topic; 2-3 leads to around 180 topics\n",
    "    nr_topics = 100, # number of topics; theyworkforyou.com have around 90-100 topics for each MP\n",
    "    verbose = True,\n",
    "    top_n_words = 10,\n",
    "    n_gram_range = (1, 3),\n",
    "    hdbscan_model = hdbscan_model,\n",
    "    calculate_probabilities = True,\n",
    "     # https://www.sbert.net/docs/pretrained_models.html\n",
    "     # Probably saved somewhere here: /Users/aiolf1/anaconda3/envs/BERTopic/lib/python3.11/site-packages\n",
    "    embedding_model = \"paraphrase-multilingual-mpnet-base-v2\").fit(docs['all'], y = categories) # perform supervised topic modeling, we simply use all categories\n",
    "    #embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-base\")).fit(docs, y = categories) # perform supervised topic modeling, we simply use all categories\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38356c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model Information\n",
    "doc_info = topic_model.get_document_info(docs['all'])\n",
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7679c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge model output with docs in order to preserve BusinessShortNumber\n",
    "doc_info = pd.merge(doc_info, docs, left_on='Document', right_on='all', how='left')\n",
    "\n",
    "# Add BusinessShortNumber to probs for export\n",
    "probs_df = pd.DataFrame(probs)\n",
    "probs_df[\"BusinessShortNumber\"] = doc_info[\"BusinessShortNumber\"]\n",
    "\n",
    "# Remove duplicates (for testing purposes)\n",
    "doc_info = doc_info.drop_duplicates(subset='BusinessShortNumber')\n",
    "probs_df = probs_df.drop_duplicates(subset='BusinessShortNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbcafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140 topics.\n"
     ]
    }
   ],
   "source": [
    "# Number of topics\n",
    "num_topics = topic_info.shape[0]\n",
    "print(f\"There are {num_topics} topics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e5972",
   "metadata": {},
   "source": [
    "## Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea15513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "topic_model.save(\"BERT_data/v230827/topic_model\", serialization=\"safetensors\", save_ctfidf=True)\n",
    "\n",
    "# Save topics and probs to file\n",
    "np.save('BERT_data/v230827/probs.npy', probs)\n",
    "np.save('BERT_data/v230827/topics.npy', topics)\n",
    "\n",
    "# Save ndarray as csv file\n",
    "probs_df.to_csv('BERT_data/v230827/probs.csv', index=False)\n",
    "\n",
    "# Save document level information to csv\n",
    "doc_info.to_csv('BERT_data/v230827/doc_info.csv', index=False)\n",
    "\n",
    "# Save topic information to csv\n",
    "topic_info.to_csv('BERT_data/v230827/topic_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9df11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "topic_model = BERTopic.load(\"BERT_data/v230820/topic_model\")\n",
    "\n",
    "# Load topics and probs\n",
    "probs = np.load('BERT_data/v230820/probs.npy', allow_pickle=True)\n",
    "topics = np.load('BERT_data/v230820/topics.npy', allow_pickle=True)\n",
    "topics = topics.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
