{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218a77ae",
   "metadata": {},
   "source": [
    "# Plain Vanilla BERTopic\n",
    "https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aae420",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd5d2b-fd9b-41cd-a29c-5ace51820c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore NumbaDeprecationWarning\n",
    "import numba\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=numba.NumbaDeprecationWarning)\n",
    "\n",
    "from bertopic import BERTopic\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4bc4f",
   "metadata": {},
   "source": [
    "## Import and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d52a5-0c0e-4375-bf49-5d325de7527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Businesses TSV as list of strings\n",
    "with open('all_businesses.tsv', newline='') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    docs = [item.replace('\\xa0', ' ') for sublist in reader for item in sublist]\n",
    "\n",
    "# Remove punctuation\n",
    "docs = [re.sub(r'[^\\w\\s]', '', doc) for doc in docs]\n",
    "\n",
    "# Lowercase\n",
    "docs = [doc.lower() for doc in docs]\n",
    "\n",
    "# Remove stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Import custom stopwords file as list of strings\n",
    "with open('../../data/custom_stopwords.txt', 'r') as f:\n",
    "   custom_stopwords = f.readlines()\n",
    "\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "custom_stopwords = [x.strip() for x in custom_stopwords]\n",
    "\n",
    "# remove stopwords from docs\n",
    "docs = [' '.join(word for word in doc.lower().split() if word not in german_stop_words) for doc in docs]\n",
    "docs = [' '.join(word for word in doc.lower().split() if word not in custom_stopwords) for doc in docs]\n",
    "\n",
    "# remove \"na\" from docs\n",
    "docs = [doc for doc in docs if doc != \"na\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29262b3-60ae-438a-8467-14fc6b548613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insepct Data\n",
    "\n",
    "# print head of docs\n",
    "print(docs[:2])\n",
    "\n",
    "# print size of docs\n",
    "print(len(docs)) # 18846"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514b5a8",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d83821-afbe-4c5c-a86e-0a19881291fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#hdbscan\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size = 2,\n",
    "    metric = 'euclidean',\n",
    "    prediction_data = True)\n",
    "\n",
    "\n",
    "# BERTopic German model\n",
    "# Parameter tuning: https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#bertopic\n",
    "topic_model = BERTopic(\n",
    "    language = \"multilingual\",\n",
    "    min_topic_size = 2,\n",
    "    verbose = True,\n",
    "    top_n_words = 20,\n",
    "    n_gram_range = (1, 2),\n",
    "    #calculate_probabilities = True, # turn on later again to calc probs\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model = \"distiluse-base-multilingual-cased-v1\") # https://www.sbert.net/docs/pretrained_models.html\n",
    "#.fit(docs)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec315277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "num_topics = topic_info.shape[0]\n",
    "print(f\"There are {num_topics} topics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fe50a",
   "metadata": {},
   "source": [
    "##Â Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"topic_model.get_topic_info()\")\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da61f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"topic_model.get_topic(0)\")\n",
    "print(topic_model.get_topic(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract information on a document level\n",
    "#print(\"topic_model.get_document_info(docs)\")\n",
    "#print(topic_model.get_document_info(docs))\n",
    "\n",
    "# save document level information to csv\n",
    "doc_level_info = topic_model.get_document_info(docs)\n",
    "\n",
    "doc_level_info.to_csv('doc_level_info.csv', index=False)\n",
    "\n",
    "# save doc_level_info to csv\n",
    "# with open('doc_level_info.csv', 'w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(doc_level_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the \"Representation\" column\n",
    "\n",
    "unique_values_representation = doc_level_info['Representation']\n",
    "\n",
    "type(unique_values_representation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37528d9b",
   "metadata": {},
   "source": [
    "## Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_distr, _ = topic_model.approximate_distribution(docs)\n",
    "#print(topic_distr)\n",
    "\n",
    "# print dimension of probs\n",
    "#print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54812846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export topic_distribution as CSV\n",
    "with open('topic_distribution_test.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    #writer.writerows(topic_distr)\n",
    "    writer.writerows(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c17393",
   "metadata": {},
   "source": [
    "## Hierarchical Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a011b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
