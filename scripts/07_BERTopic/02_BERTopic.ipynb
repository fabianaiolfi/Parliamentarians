{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453ea87e",
   "metadata": {},
   "source": [
    "# Semi-supervised BERTopic Modeling with Probability Calculation\n",
    "https://maartengr.github.io/BERTopic/getting_started/semisupervised/semisupervised.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aae420",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd5d2b-fd9b-41cd-a29c-5ace51820c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=numba.NumbaDeprecationWarning)\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4bc4f",
   "metadata": {},
   "source": [
    "## Import and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_businesses.tsv', sep='\\t', dtype={'BusinessShortNumber': str})\n",
    "\n",
    "\n",
    "# Split text and TagNames\n",
    "docs = df[[\"BusinessShortNumber\", \"all\"]]\n",
    "categories = df[\"TagNames\"]\n",
    "\n",
    "# Convert categories to numbers\n",
    "categories = pd.factorize(categories)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d52a5-0c0e-4375-bf49-5d325de7527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "#docs = [re.sub(r'[^\\w\\s]', '', doc) for doc in docs]\n",
    "docs['all'] = docs['all'].apply(lambda doc: re.sub(r'[^\\w\\s]', '', doc))\n",
    "\n",
    "\n",
    "# Lowercase\n",
    "#docs = [doc.lower() for doc in docs]\n",
    "docs['all'] = docs['all'].apply(lambda doc: doc.lower())\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Import custom stopwords file as list of strings\n",
    "with open('../../data/custom_stopwords.txt', 'r') as f:\n",
    "   custom_stopwords = f.readlines()\n",
    "\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "custom_stopwords = [x.strip() for x in custom_stopwords]\n",
    "\n",
    "# remove stopwords from docs\n",
    "#docs = [' '.join(word for word in doc.lower().split() if word not in german_stop_words) for doc in docs]\n",
    "#docs = [' '.join(word for word in doc.lower().split() if word not in custom_stopwords) for doc in docs]\n",
    "docs['all'] = docs['all'].apply(lambda doc: ' '.join(word for word in doc.split() if word not in german_stop_words))\n",
    "docs['all'] = docs['all'].apply(lambda doc: ' '.join(word for word in doc.split() if word not in custom_stopwords))\n",
    "\n",
    "# remove \"na\" from docs\n",
    "#docs = [doc for doc in docs if doc != \"na\"]\n",
    "docs = docs[docs['all'] != 'na']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514b5a8",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d83821-afbe-4c5c-a86e-0a19881291fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#hdbscan\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size = 2,\n",
    "    min_samples = 1,\n",
    "    metric = 'euclidean',\n",
    "    prediction_data = True)\n",
    "\n",
    "# BERTopic German model\n",
    "# Parameter tuning: https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#bertopic\n",
    "topic_model = BERTopic(\n",
    "    language = \"multilingual\",\n",
    "    min_topic_size = 2,\n",
    "    verbose = True,\n",
    "    top_n_words = 10,\n",
    "    n_gram_range = (1, 3),\n",
    "    hdbscan_model = hdbscan_model,\n",
    "    calculate_probabilities = True,\n",
    "     # https://www.sbert.net/docs/pretrained_models.html\n",
    "    embedding_model = \"paraphrase-multilingual-mpnet-base-v2\").fit(docs, y = categories) # perform supervised topic modeling, we simply use all categories\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38356c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model Information\n",
    "doc_info = topic_model.get_document_info(docs)\n",
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7679c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge model output with docs in order to preserve BusinessShortNumber\n",
    "doc_info = pd.merge(doc_info, docs, left_on='Document', right_on='all', how='left')\n",
    "\n",
    "# Add BusinessShortNumber to probs for export\n",
    "probs_df = pd.DataFrame(probs)\n",
    "probs_df[\"BusinessShortNumber\"] = doc_info[\"BusinessShortNumber\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bbcafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 147 topics.\n"
     ]
    }
   ],
   "source": [
    "# Number of topics\n",
    "num_topics = topic_info.shape[0]\n",
    "print(f\"There are {num_topics} topics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e5972",
   "metadata": {},
   "source": [
    "## Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ea15513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "topic_model.save(\"BERT_data/temp/topic_model\", serialization=\"safetensors\", save_ctfidf=True)\n",
    "\n",
    "# Save topics and probs to file\n",
    "np.save('BERT_data/temp/probs.npy', probs)\n",
    "np.save('BERT_data/temp/topics.npy', topics)\n",
    "\n",
    "# Save ndarray as csv file\n",
    "probs_df.to_csv('BERT_data/temp/probs.csv', index=False)\n",
    "\n",
    "# Save document level information to csv\n",
    "doc_info.to_csv('BERT_data/temp/doc_info.csv', index=False)\n",
    "\n",
    "# Save topic information to csv\n",
    "topic_info.to_csv('BERT_data/temp/topic_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9df11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "topic_model = BERTopic.load(\"BERT_data/v230820/topic_model\")\n",
    "\n",
    "# Load topics and probs\n",
    "probs = np.load('BERT_data/v230820/probs.npy', allow_pickle=True)\n",
    "topics = np.load('BERT_data/v230820/topics.npy', allow_pickle=True)\n",
    "topics = topics.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
