# Model Documentation

## v230827

**Sumaries**
ChatGPT gpt-4

**Tags**
ChatGPT gpt-4

**Pre-processing for BERTopic**
â€¦?

**Cutoff** 03_post_process_BERT.R
```
# Multiple topics per Business Item ---------------------------------------------------------------

top_topics <- topic_probs %>%
  select(top_3_topics, prob_1, prob_2, prob_3) %>% 
  separate(top_3_topics, into = c("topic_1", "topic_2", "topic_3"), sep = ",") %>%
  mutate_at(vars(topic_1:topic_3), ~as.integer(sub("X", "", .))) %>% 
  # Re-align topic numbers with topic number generated in BERTopic (topic nr 1 -> topic nr 0)
  mutate(topic_1 = topic_1 - 1) %>% mutate(topic_2 = topic_2 - 1) %>% mutate(topic_3 = topic_3 - 1) %>% 
  mutate(topic_2 = case_when(prob_2 < 0.001 ~ NA,
                             prob_2 >= 0.001 ~ topic_2)) %>% 
  mutate(topic_3 = case_when(is.na(topic_2) == TRUE ~ NA,
                             is.na(topic_2) == FALSE ~ topic_3)) %>% 
  mutate(prob_2 = case_when(prob_2 < 0.001 ~ NA,
                             prob_2 >= 0.001 ~ prob_2)) %>% 
  mutate(prob_3 = case_when(is.na(prob_2) == TRUE ~ NA,
                             is.na(prob_2) == FALSE ~ prob_3))
```

**BERTopic Modelling**
```
# https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#hdbscan
hdbscan_model = HDBSCAN(
    min_cluster_size = 2,
    min_samples = 1,
    metric = 'euclidean',
    prediction_data = True)

# BERTopic German model
# Parameter tuning: https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#bertopic
topic_model = BERTopic(
    language = "multilingual",
    min_topic_size = 4, # minimum number of documents in a topic; 2-3 leads to around 180 topics
    nr_topics = 100, # number of topics; theyworkforyou.com have around 90-100 topics for each MP
    verbose = True,
    top_n_words = 10,
    n_gram_range = (1, 3),
    hdbscan_model = hdbscan_model,
    calculate_probabilities = True,
     # https://www.sbert.net/docs/pretrained_models.html
     # Probably saved somewhere here: /Users/aiolf1/anaconda3/envs/BERTopic/lib/python3.11/site-packages
    embedding_model = "paraphrase-multilingual-mpnet-base-v2").fit(docs['all'], y = categories) # perform supervised topic modeling, we simply use all categories
    #embedding_model = SentenceTransformer("intfloat/multilingual-e5-base")).fit(docs, y = categories) # perform supervised topic modeling, we simply use all categories
```

**Topic Names**
- Generated using Title and summaries produced by BERTopic

Automated Document Intrusion Detection
[1] "Accuracy:  
[1] "Precision (Mean over all Topics): 
[1] "Recall (Mean over all Topics): 
[1] "F1 Score (Mean over all Topics):